# -*- coding: utf-8 -*-
"""manual_specinfer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RAJlJI5tzHHQLqUapNcbytR2X1nWujNZ
"""

# ! pip install transformers datasets

# ! pip install torch

"""sentencepice and"""

# ! pip install sentencepiece

# ! mkdir logs
# ! mkdir stats

# Initialize SSMs and LLMs
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig
import torch
from tqdm import tqdm
import json
import torch.nn.functional as F

# general configs
DATASET_PATH = "alpaca.json"
TREE_DEPTH = 4 # tokens generated by SSM in one pass
MAX_GENERATION_LEN = 24 # max token to generate by LLM
CACHE_DIR = ".cache" # change to your dir
LOG_PATH = "logs/log_alpaca.txt"  # remember to mkdir logs
STATISTIC_PATH = "stats/stats_alpaca.txt" # remember to mkdir stats
DEVICE = "cuda:0" if torch.cuda.is_available() else "cpu"
NULL_TOKEN = -1
BRANCHING_FACTOR = 2
print(DEVICE)

# Find and set suitable generation configs for SSMs
ssm_generation_configs = [
    GenerationConfig(
        do_sample=True, temperature=0.5
    ),
    # GenerationConfig(
    #     do_sample=True, temperature=0.5
    # ),
    # GenerationConfig(
    #     do_sample=True, temperature=0.5
    # ),
    # GenerationConfig(
    #     do_sample=True, temperature=0.5
    # )
]

ssm_tokenizers = [
    # AutoTokenizer.from_pretrained("JackFram/llama-68m", cache_dir=CACHE_DIR),
    # AutoTokenizer.from_pretrained("JackFram/llama-160m", cache_dir=CACHE_DIR),
    # AutoTokenizer.from_pretrained("facebook/opt-125m", cache_dir=CACHE_DIR),
    AutoTokenizer.from_pretrained("facebook/opt-125m", cache_dir=CACHE_DIR)
]

ssm_models = [
    # AutoModelForCausalLM.from_pretrained("JackFram/llama-68m", cache_dir=CACHE_DIR).to(DEVICE),
    # AutoModelForCausalLM.from_pretrained("JackFram/llama-160m", cache_dir=CACHE_DIR).to(DEVICE),
    # AutoModelForCausalLM.from_pretrained("facebook/opt-125m", cache_dir=CACHE_DIR).to(DEVICE),
    AutoModelForCausalLM.from_pretrained("facebook/opt-125m", cache_dir=CACHE_DIR).to(DEVICE)
]

# TODO Find and set suitable generation config for LLM
llm_generation_config = GenerationConfig(do_sample=True, temperature=0.1)
llm_tokenizer = AutoTokenizer.from_pretrained("facebook/opt-6.7b", cache_dir=CACHE_DIR)
llm_model = AutoModelForCausalLM.from_pretrained("facebook/opt-6.7b", cache_dir=CACHE_DIR).to(DEVICE)

"""
write the output to a file
"""
def log_print(*args, file_path=LOG_PATH, sep=' ', end='\n'):
    """Custom print function that writes to a specified file."""
    with open(file_path, 'a') as f:
        print(*args, file=f, sep=sep, end=end)

"""
TokenTree

Main class that can construct token trees, find the verified sequence against an
LLM and find statistics of the tree
"""
class TokenTree():
    def __init__(self, root_token=NULL_TOKEN):
        self.root = root_token
        self.children = []

    def get_token(self):
        return self.root

    def add_child(self, token):
        self.children.append(TokenTree(token))

    def delete_child(self, token):
        for i, child in enumerate(self.children):
            if child.get_token() == token:
                self.children.pop(i)

    """
    construct_from_chains

    Takes list of completions, where each completion is a list of token ids.
    Constructs a merged token tree

    Params:
        chains: list of list of tokens. Each list of tokens is a completion by an SSM
    """
    def construct_from_chains(self, chains):
        chains = [chain for chain in chains if len(chain) >= 1]
        level_tokens = [chain[0] for chain in chains]
        unique_child_tokens = list(set(level_tokens))

        for child_token in unique_child_tokens:
            self.add_child(child_token)
            child_tree = self.children[-1]
            child_chains = [chain[1:] for i, chain in enumerate(chains) if chain[0] == child_token]
            child_tree.construct_from_chains(child_chains)

    def display_tree(self, level=0):
        # DFS
        if level == 0:
            log_print("Displaying tree:\n")
        log_print("\t"*level + f"{self.get_token()}")
        for child_tree in self.children:
            child_tree.display_tree(level+1)

    """
    longest_matching_path

    Given a token path (generated by LLM), compare against tree and find longest
    prefix of the path that exists in the tree. This corresponds to tokens
    'verified' by the LLM

    Params:
        token_path: list of token ids (generated by LLM)
    """
    def longest_matching_path(self, token_path):
        if len(token_path) > 0:
            curr_token = token_path[0]
            for child in self.children:
                if child.get_token() == curr_token:
                    return [curr_token] + child.longest_matching_path(token_path[1:])
            return []
        else:
            return []

    """
    node_count

    Returns number of nodes in tree, including root
    """
    def node_count(self):
        nc = 1
        for child in self.children:
            nc += child.node_count()
        return nc

    """
    height_sum

    Returns sum of heights (depths actually) of all nodes, assuming the root
    is at height 0 and its children are at height 1 and so on
    """
    def height_sum(self):
        hs = 0
        for child in self.children:
            hs += child.height_sum() + child.node_count()
        return hs

    def average_height(self):
        return self.height_sum()/self.node_count()

    """
    abs_dev_sum

    Balance metric for the tree.
    Returns the average absolute deviation of node heights from the average height

    Params
        avg_height: If None, it will internally recompute. None should be used
                    only when calling at the root
        curr_height: Height of the root
    """
    def abs_dev_sum(self, avg_height=None, curr_height=0):
        if avg_height is None:
            avg_height = self.average_height()

        score = abs(curr_height-avg_height)
        for child in self.children:
            score += child.abs_dev_sum(avg_height, curr_height+1)

        return score

    def avg_abs_dev_sum(self):
        return self.abs_dev_sum()/self.node_count()

    def balance_score(self):
        return self.avg_abs_dev_sum()

with open(DATASET_PATH, 'r') as file:
    dataset = json.load(file)
print(len(dataset))

"""
generate_extend_token_tree

Given token tree (possibly None), extend it by taking top k tokens at each step, k=branching_factor
"""
SSM_CALLS = 0

def get_top_k_tokens(prompt, model, tokenizer, k=2):
    global SSM_CALLS
    inputs = tokenizer(prompt, return_tensors='pt').to(DEVICE)
    outputs = model(**inputs)
    SSM_CALLS += 1
    # If you are not on a source install, replace outputs.logits by outputs[0]
    predictions = F.softmax(outputs.logits, dim=-1)

    ids = torch.argsort(predictions[0][-1], descending=True)[:k]
    return ids

def extend_token_tree_once(prompt, tt, model, tokenizer, branching_factor=2):
    # Go to all leafs
    if len(tt.children) == 0: # I am leaf
        # Generate
        ids = get_top_k_tokens(prompt, model, tokenizer, branching_factor)

        # add to children
        for child_token_id in ids:
            tt.children.append(TokenTree(child_token_id))

    else: # I am not leaf so call on my children
        for child in tt.children:
            # Construct effective prompt
            prompt = prompt + tokenizer.decode([child.get_token()]) if child.get_token() != NULL_TOKEN else prompt
            # Recurse on child
            extend_token_tree_once(prompt, child, model, tokenizer, branching_factor)

    return tt

def generate_branching_token_tree(prompt, tt, model, tokenizer, depth=TREE_DEPTH, branching_factor=2):
    # Call extend_token_tree_once depth times
    for i in range(depth):
        if i > 2:
            # Don't branch
            tt = extend_token_tree_once(prompt, tt, model, tokenizer, branching_factor=1)
        else:
            # Happens in-place on tt, but you can reassign on LHS if you wish
            tt = extend_token_tree_once(prompt, tt, model, tokenizer, branching_factor)
    return tt

def merge_two_tt(tt1, tt2):
    assert(tt1.get_token() == tt2.get_token())

    tt1_child_tokens_to_tt = {t.get_token():[i,t] for i, t in enumerate(tt1.children)}

    for child in tt2.children:
        child_token = child.get_token()
        if child_token not in tt1_child_tokens_to_tt:
            tt1.children.append(child)
        else:
            tt1.children.append(merge_two_tt(tt1_child_tokens_to_tt[child_token][1], child))
            # remove old index
            tt1.children.pop(tt1_child_tokens_to_tt[child_token][0])

    return tt1

def merge_tt_lst(tt_lst):
    if len(tt_lst) == 0:
        log_print("Empty tt lst")
        return None

    curr_tt = tt_lst[0]
    for i in range(1, len(tt_lst)):
        curr_tt = merge_two_tt(curr_tt, tt_lst[i])

    return curr_tt

## MERGE TESTS
# testing_tt1 = TokenTree()
# testing_tt1.children = [TokenTree(25), TokenTree(39)]
# testing_tt1.children[0].children = [TokenTree(99), TokenTree(97)]
# testing_tt1.display_tree()


# testing_tt2 = TokenTree()
# testing_tt2.children = [TokenTree(25), TokenTree(44)]
# testing_tt2.children[0].children = [TokenTree(99), TokenTree(98)]
# testing_tt2.display_tree()


# testing_tt3 = TokenTree()
# testing_tt3.children = [TokenTree(44), TokenTree(37)]
# testing_tt3.children[0].children = [TokenTree(34), TokenTree(33)]
# testing_tt3.display_tree()


# merged_testing_tt = merge_tt_lst([testing_tt1, testing_tt2, testing_tt3])
# merged_testing_tt.display_tree()

## GENERATION TESTS
# print(get_top_k_tokens("Hey are you conscious?", ssm_models[0], ssm_tokenizers[0], 2))
# testing_tt1 = TokenTree()
# testing_tt1.display_tree()
# generate_branching_token_tree("Hey are you conscious?", testing_tt1, ssm_models[0], ssm_tokenizers[0], depth=4, branching_factor=2)

# testing_tt1.display_tree()
# extend_token_tree_once("Hey, are you conscious?", testing_tt1,
#                        ssm_models[0], ssm_tokenizers[0], branching_factor=2)



# testing_tt2 = TokenTree()
# testing_tt2.children = [TokenTree(25), TokenTree(44)]
# testing_tt2.children[0].children = [TokenTree(99), TokenTree(98)]
# testing_tt2.display_tree()


# testing_tt3 = TokenTree()
# testing_tt3.children = [TokenTree(44), TokenTree(37)]
# testing_tt3.children[0].children = [TokenTree(34), TokenTree(33)]
# testing_tt3.display_tree()


# merged_testing_tt = merge_tt_lst([testing_tt1, testing_tt2, testing_tt3])
# merged_testing_tt.display_tree()

# prompts = ["Hey, are you conscious?"]
prompts = dataset[:100]

for orig_prompt in tqdm(prompts):
    prompt = orig_prompt
    # Generate with LLM
    llm_inputs = llm_tokenizer(prompt, return_tensors="pt")
    llm_inputs.to(DEVICE)
    input_token_length = len(llm_inputs.input_ids[0])
    len_total = input_token_length
    len_here = int(1e6)
    llm_generate_ids = llm_model.generate(llm_inputs.input_ids, max_new_tokens=MAX_GENERATION_LEN + TREE_DEPTH).clone().detach().cpu().numpy()
    llm_res = llm_tokenizer.batch_decode(llm_generate_ids, skip_special_tokens=True)[0]


    # Keep doing SSM speculation till max generation len is reached or llm finishes generating
    while len_total < len(llm_generate_ids[0]):
        # Generate SSM completions
        ssm_gen_ids_lst = []
        tt_lst = []
        SSM_CALLS = 0
        for i in range(len(ssm_models)):
            log_print(f"{i}th SSM")
            tt_lst.append(generate_branching_token_tree(prompt, TokenTree(), ssm_models[i], ssm_tokenizers[i], depth=TREE_DEPTH, branching_factor=BRANCHING_FACTOR))
            tt_lst[-1].display_tree()

        log_print(f"SSMs were called {SSM_CALLS} times")
        # Merge all SSM trees
        tt = merge_tt_lst(tt_lst)

        # Find longest path through SSM tree that coincides with LLM (number of verified tokens)
        log_print("Merged tree:")
        tt.display_tree()
        lmp = tt.longest_matching_path(llm_generate_ids[0, len_total: len_total + TREE_DEPTH])
        log_print("Tree balance score:", tt.balance_score(), file_path=LOG_PATH)

        # Record the statistics
        len_here = 0 if not lmp else len(lmp)
        log_print("longest matching path, length:", len_here, file_path=LOG_PATH)
        log_print(tt.balance_score(), len_here, SSM_CALLS, file_path=STATISTIC_PATH)

        # if there's no verified token, append next token
        if not lmp:
            lmp = llm_generate_ids[0, len_total]
        len_total += max(len_here, 1)

        llm_decoded_str = llm_tokenizer.batch_decode(torch.tensor([lmp], dtype=int), skip_special_tokens=True, clean_up_tokenization_spaces=True)[0]
        log_print("longest matching path decoded:", llm_decoded_str, file_path=LOG_PATH)
        prompt += llm_decoded_str
        log_print("current prompt: ", prompt, file_path=LOG_PATH)

    log_print("generated text for this prompt:", prompt, file_path=LOG_PATH)
