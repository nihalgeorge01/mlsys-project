{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3484e4da4d1490c81d6ea595f86cad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize SSMs and LLMs\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# general configs\n",
    "DATASET_PATH = \"alpaca.json\"\n",
    "TREE_DEPTH = 8 # tokens generated by SSM in one pass\n",
    "MAX_GENERATION_LEN = 24 # max token to generate by LLM\n",
    "CACHE_DIR = \".cache\" # change to your dir\n",
    "LOG_PATH = \"logs/log_alpaca.txt\"  # remember to mkdir logs\n",
    "STATISTIC_PATH = \"stats/stats_alpaca.txt\" # remember to mkdir stats\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)\n",
    "# TODO Find and set suitable generation configs for SSMs\n",
    "ssm_generation_configs = [\n",
    "    GenerationConfig(\n",
    "        do_sample=True, temperature=0.5\n",
    "    ),\n",
    "    GenerationConfig(\n",
    "        do_sample=True, temperature=0.5\n",
    "    ),\n",
    "    GenerationConfig(\n",
    "        do_sample=True, temperature=0.5\n",
    "    ),\n",
    "    GenerationConfig(\n",
    "        do_sample=True, temperature=0.5\n",
    "    )\n",
    "]\n",
    "\n",
    "ssm_tokenizers = [\n",
    "    AutoTokenizer.from_pretrained(\"JackFram/llama-68m\", cache_dir=CACHE_DIR),\n",
    "    AutoTokenizer.from_pretrained(\"JackFram/llama-160m\", cache_dir=CACHE_DIR),\n",
    "    AutoTokenizer.from_pretrained(\"facebook/opt-125m\", cache_dir=CACHE_DIR),\n",
    "    AutoTokenizer.from_pretrained(\"facebook/opt-125m\", cache_dir=CACHE_DIR)\n",
    "]\n",
    "\n",
    "ssm_models = [\n",
    "    AutoModelForCausalLM.from_pretrained(\"JackFram/llama-68m\", cache_dir=CACHE_DIR).to(DEVICE),\n",
    "    AutoModelForCausalLM.from_pretrained(\"JackFram/llama-160m\", cache_dir=CACHE_DIR).to(DEVICE),\n",
    "    AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\", cache_dir=CACHE_DIR).to(DEVICE),\n",
    "    AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\", cache_dir=CACHE_DIR).to(DEVICE)\n",
    "]\n",
    "\n",
    "# TODO Find and set suitable generation config for LLM\n",
    "llm_generation_config = GenerationConfig(do_sample=True, temperature=0.1)\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-6.7b\", cache_dir=CACHE_DIR)\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-6.7b\", cache_dir=CACHE_DIR).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "write the output to a file\n",
    "\"\"\"\n",
    "def log_print(*args, file_path=LOG_PATH, sep=' ', end='\\n'):\n",
    "    \"\"\"Custom print function that writes to a specified file.\"\"\"\n",
    "    with open(file_path, 'a') as f:\n",
    "        print(*args, file=f, sep=sep, end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TokenTree\n",
    "\n",
    "Main class that can construct token trees, find the verified sequence against an\n",
    "LLM and find statistics of the tree\n",
    "\"\"\"\n",
    "class TokenTree():\n",
    "    def __init__(self, root_token=0):\n",
    "        self.root = root_token\n",
    "        self.children = []\n",
    "\n",
    "    def get_token(self):\n",
    "        return self.root\n",
    "\n",
    "    def add_child(self, token):\n",
    "        self.children.append(TokenTree(token))\n",
    "\n",
    "    def delete_child(self, token):\n",
    "        for i, child in enumerate(self.children):\n",
    "            if child.get_token() == token:\n",
    "                self.children.pop(i)\n",
    "\n",
    "    \"\"\"\n",
    "    construct_from_chains\n",
    "\n",
    "    Takes list of completions, where each completion is a list of token ids.\n",
    "    Constructs a merged token tree\n",
    "\n",
    "    Params:\n",
    "        chains: list of list of tokens. Each list of tokens is a completion by an SSM\n",
    "    \"\"\"\n",
    "    def construct_from_chains(self, chains):\n",
    "        chains = [chain for chain in chains if len(chain) >= 1]\n",
    "        level_tokens = [chain[0] for chain in chains]\n",
    "        unique_child_tokens = list(set(level_tokens))\n",
    "\n",
    "        for child_token in unique_child_tokens:\n",
    "            self.add_child(child_token)\n",
    "            child_tree = self.children[-1]\n",
    "            child_chains = [chain[1:] for i, chain in enumerate(chains) if chain[0] == child_token]\n",
    "            child_tree.construct_from_chains(child_chains)\n",
    "\n",
    "    def display_tree(self, level=0):\n",
    "        # DFS\n",
    "        if level == 0:\n",
    "            log_print(\"Displaying tree:\\n\")\n",
    "        log_print(\"\\t\"*level + f\"{self.get_token()}\")\n",
    "        for child_tree in self.children:\n",
    "            child_tree.display_tree(level+1)\n",
    "\n",
    "    \"\"\"\n",
    "    longest_matching_path\n",
    "\n",
    "    Given a token path (generated by LLM), compare against tree and find longest\n",
    "    prefix of the path that exists in the tree. This corresponds to tokens\n",
    "    'verified' by the LLM\n",
    "\n",
    "    Params:\n",
    "        token_path: list of token ids (generated by LLM)\n",
    "    \"\"\"\n",
    "    def longest_matching_path(self, token_path):\n",
    "        if len(token_path) > 0:\n",
    "            curr_token = token_path[0]\n",
    "            for child in self.children:\n",
    "                if child.get_token() == curr_token:\n",
    "                    return [curr_token] + child.longest_matching_path(token_path[1:])\n",
    "            return []\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    \"\"\"\n",
    "    node_count\n",
    "\n",
    "    Returns number of nodes in tree, including root\n",
    "    \"\"\"\n",
    "    def node_count(self):\n",
    "        nc = 1\n",
    "        for child in self.children:\n",
    "            nc += child.node_count()\n",
    "        return nc\n",
    "\n",
    "    \"\"\"\n",
    "    height_sum\n",
    "\n",
    "    Returns sum of heights (depths actually) of all nodes, assuming the root\n",
    "    is at height 0 and its children are at height 1 and so on\n",
    "    \"\"\"\n",
    "    def height_sum(self):\n",
    "        hs = 0\n",
    "        for child in self.children:\n",
    "            hs += child.height_sum() + child.node_count()\n",
    "        return hs\n",
    "\n",
    "    def average_height(self):\n",
    "        return self.height_sum()/self.node_count()\n",
    "\n",
    "    \"\"\"\n",
    "    abs_dev_sum\n",
    "\n",
    "    Balance metric for the tree.\n",
    "    Returns the average absolute deviation of node heights from the average height\n",
    "\n",
    "    Params\n",
    "        avg_height: If None, it will internally recompute. None should be used\n",
    "                    only when calling at the root\n",
    "        curr_height: Height of the root\n",
    "    \"\"\"\n",
    "    def abs_dev_sum(self, avg_height=None, curr_height=0):\n",
    "        if avg_height is None:\n",
    "            avg_height = self.average_height()\n",
    "\n",
    "        score = abs(curr_height-avg_height)\n",
    "        for child in self.children:\n",
    "            score += child.abs_dev_sum(avg_height, curr_height+1)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def avg_abs_dev_sum(self):\n",
    "        return self.abs_dev_sum()/self.node_count()\n",
    "\n",
    "    def balance_score(self):\n",
    "        return self.avg_abs_dev_sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "with open(DATASET_PATH, 'r') as file:\n",
    "    dataset = json.load(file)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/1000 [04:19<13:02:11, 47.21s/it]"
     ]
    }
   ],
   "source": [
    "# prompts = [\"Hey, are you conscious?\"]\n",
    "prompts = dataset\n",
    "\n",
    "for curr_prompt in tqdm(prompts):\n",
    "    prompt = curr_prompt\n",
    "    # Generate with LLM \n",
    "    llm_inputs = llm_tokenizer(prompt, return_tensors=\"pt\")\n",
    "    llm_inputs.to(DEVICE)\n",
    "    input_token_length = len(llm_inputs.input_ids[0])\n",
    "    len_total = input_token_length\n",
    "    len_here = int(1e6)\n",
    "    llm_generate_ids = llm_model.generate(llm_inputs.input_ids, max_new_tokens=MAX_GENERATION_LEN + TREE_DEPTH).clone().detach().cpu().numpy()\n",
    "    llm_res = llm_tokenizer.batch_decode(llm_generate_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "\n",
    "    # Keep doing SSM speculation till max generation len is reached or llm finishes generating\n",
    "    while len_total < len(llm_generate_ids[0]) :\n",
    "        # Generate SSM completions\n",
    "        ssm_gen_ids_lst = []\n",
    "        for i in range(len(ssm_models)):\n",
    "            ssm_inputs = ssm_tokenizers[i](prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "            ssm_generate_ids = ssm_models[i].generate(ssm_inputs.input_ids, max_new_tokens=TREE_DEPTH, generation_config=ssm_generation_configs[i]).clone().detach().cpu().numpy()\n",
    "            ssm_gen_ids_lst.append(ssm_generate_ids[0,-TREE_DEPTH:])\n",
    "            \n",
    "\n",
    "        # Find longest path through SSM tree that coincides with LLM (number of verified tokens)\n",
    "        tt = TokenTree()\n",
    "        tt.construct_from_chains(ssm_gen_ids_lst)\n",
    "        tt.display_tree()\n",
    "        lmp = tt.longest_matching_path(llm_generate_ids[0, len_total: len_total + TREE_DEPTH])\n",
    "        log_print(\"Tree balance score:\", tt.balance_score(), file_path=LOG_PATH)\n",
    "        \n",
    "        # Record the statistics\n",
    "        len_here = 0 if not lmp else len(lmp)\n",
    "        log_print(\"longest matching path, length:\", len_here, file_path=LOG_PATH)\n",
    "        log_print(tt.balance_score(), len_here, file_path=STATISTIC_PATH)\n",
    "\n",
    "        # if there's no verified token, append next token\n",
    "        if not lmp:\n",
    "            lmp = llm_generate_ids[0, len_total]\n",
    "        len_total += max(len_here, 1)\n",
    "\n",
    "        llm_decoded_str = llm_tokenizer.batch_decode(torch.tensor([lmp], dtype=int), skip_special_tokens=True, clean_up_tokenization_spaces=True)[0]\n",
    "        log_print(\"longest matching path decoded:\", llm_decoded_str, file_path=LOG_PATH)\n",
    "        prompt += llm_decoded_str\n",
    "        log_print(\"current prompt: \", prompt, file_path=LOG_PATH)\n",
    "\n",
    "    log_print(\"generated text for this prompt:\", prompt, file_path=LOG_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "15442",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
